#  H&M Product Data Pipeline

A complete **ETL pipeline** that scrapes, cleans, and stores product data from **H&M.com** using **Apache Airflow** orchestration.

## Project Overview

This project implements an automated data pipeline that:
- **Scrapes** product listings from H&M (dynamic website with JavaScript rendering and infinite scroll)
- **Cleans** the data (removes duplicates, handles missing values, normalizes fields)
- **Loads** the cleaned data into a SQLite database
- **Orchestrates** the entire workflow using Apache Airflow (runs daily)

**Website:** [H&M](https://www2.hm.com)  
**Technology:** Selenium WebDriver for JavaScript-rendered content  
**Target Dataset:** 100+ products with structured fields (brand, description, price)

## Website Features

**Dynamic Catalog:** Product cards rendered via JavaScript  
**Multiple Categories:** Jeans, tops, dresses, skirts, trousers  
**Structured Fields:** Brand, description, raw price  
**Rich Metadata:** Suitable for analytics and visualization

## Why H&M?

 **Dynamic Content:** Uses JavaScript rendering for product listings  
 **Infinite Scroll:** Lazy loads more products as you scroll   **Structured Data:** Provides brand, description, and price fields  
 **High Volume:** Hundreds of products across multiple categories  
 **No Demo Conflicts:** Not used in course demonstrations

| Criteria                        | Met | Justification |
|--------------------------------|-----|---------------|
| JavaScript-rendered content    | +  | Product listings load dynamically |
| Infinite scroll / lazy loading | +  | Pages require scroll or button interaction |
| Structured product cards       | +  | Consistent fields: brand, title, price |
| High-volume data               | +  | 100+ products per category |
| Not used in course demos       | +  | Unique site selection |

## Project Structure

H&M/ 
â”œâ”€â”€ README.md 
â”œâ”€â”€ requirements.txt 
â”œâ”€â”€ airflow_dag.py 
â”‚ 
â”œâ”€â”€ src/ 
â”‚ â”œâ”€â”€ scraper.py # Selenium-based scraper 
â”‚ â”œâ”€â”€ cleaner.py # Data cleaning logic 
â”‚ â””â”€â”€ loader.py # SQLite loader 
â”‚ 
â””â”€â”€ data/ 
â”‚ â”œâ”€â”€ dirty_data.csv # Raw scraped data 
â”‚ â”œâ”€â”€ cleaned_data.csv # Cleaned output 
â”‚ â””â”€â”€ hm_data.db # Final SQLite database


## Implementation Details

1. Web Scraping (`src/scraper.py`)
**Technology:** Selenium WebDriver with Chrome

**Features:**
- **Headless browser** support for automation
- **Dynamic content handling** (waits for product cards)
- **Infinite scroll** implementation to load 100+ products
- **Popup handling** (cookie consent)
- **Duplicate filtering** during scraping

**Extracted Data Fields:**
- `brand`: Default "H&M"
- `description`: Product name
- `price_raw`: Raw price string

2. Data Cleaning (src/cleaner.py)
****Technology:** Pandas

**Cleaning Operations:**
    **1. Remove Duplicates**
 Drops exact duplicates and duplicate product IDs.
    **2.Handle Missing Values**
 Fills missing brand with "H&M" and description with "No description".
    **3.Normalize Text Fields**
 Strips whitespace, standardizes case.
    **4.Convert Data Types**
 Converts price_raw â†’ numeric price (float).
    **5.Validate Records**
 Removes rows with missing critical fields.

**Output:** cleaned_data.csv

3. Database Loading (src/loader.py)
**Technology:** SQLite3 (Python standard library)

**Database Schema:**

sql
CREATE TABLE hm_products (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    brand TEXT,
    description TEXT,
    price REAL,
    loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
**Features:**
- **Replace strategy:** Table is replaced on each run
- **Transaction support** for data integrity
- **Timestamping** via loaded_at

4. Apache Airflow DAG
**DAG ID:** hm_scraping_pipeline 
**Schedule:** @daily (runs once every 24 hours)

**Tasks:**
**1.scrape_data** â†’ Scrapes H&M using Selenium
**2.clean_data** â†’ Cleans and validates the data
**3.load_to_sqlite** â†’ Loads into SQLite database

## Verification Checklist
**Scraping**
[ ] Browser opens successfully

[ ] Page loads without errors

[ ] Cookie popup handled

[ ] Page scrolls to load more content

[ ] At least 100 products extracted

[ ] dirty_data.csv created

**Cleaning**
[ ] Duplicates removed
[ ] Missing values handled
[ ] Price converted to float
[ ] Text normalized
[ ] cleaned_data.csv created

**Loading**
[ ] SQLite database created
[ ] Table schema correct
[ ] Data inserted successfully
[ ] hm_data.db contains 100+ records

**Airflow DAG**
[ ] DAG appears in Airflow UI
[ ] Schedule set to @daily
[ ] Tasks: scrape â†’ clean â†’ load
[ ] DAG has successful run
[ ] Logs show complete execution

ğŸ“ Assignment Requirements Compliance
Requirement	Status	Details
Dynamic Website	âœ…	H&M uses JavaScript rendering, infinite scroll
Selenium/Playwright	âœ…	Implemented in scraper.py
Structured Data	âœ…	Product cards with brand, description, price
100+ Records	âœ…	Scrapes 110+ products
Remove Duplicates	âœ…	Implemented in cleaner.py
Handle Missing Values	âœ…	Default brand/description
Normalize Text	âœ…	Strip whitespace, standardize case
Type Conversions	âœ…	Price â†’ float
SQLite Database	âœ…	hm_data.db with schema
Airflow DAG	âœ…	scrape â†’ clean â†’ load
24h Schedule	âœ…	@daily
Logging & Retries	âœ…	Configured in DAG
Successful Run	âœ…	Verified via Airflow UI

## References
**H&M:** https://www2.hm.com
**Selenium Documentation:** https://selenium-python.readthedocs.io/getting-started.html#simple-usage
**Apache Airflow:** https://airflow.apache.org/docs/
**Pandas:** https://pandas.pydata.org/docs/
**SQLite:** https://www.sqlite.org/docs.html